# ğŸŒ± ESG Sustainability Analysis Dashboard

[![CI Pipeline](https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis/actions/workflows/ci.yml/badge.svg)](https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis/actions/workflows/ci.yml)
[![Deploy](https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis/actions/workflows/deploy.yml/badge.svg)](https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis/actions/workflows/deploy.yml)
[![Python](https://img.shields.io/badge/python-v3.11+-blue.svg)](https://www.python.org/downloads/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-v15+-blue.svg)](https://www.postgresql.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

A production-ready, full-stack ESG (Environmental, Social, Governance) analytics platform with ML-powered risk prediction, interactive dashboards, and comprehensive API. Built with modern DevOps practices, security best practices, and scalability in mind.

## ğŸŒŸ Key Features

### Analytics & Insights
- ğŸ“Š Company-level ESG score analysis (Total ESG + E/S/G sub-scores)
- ğŸ¢ Sector and industry benchmarking
- âš ï¸ Controversy risk assessment and tracking
- ğŸ“ˆ Interactive dashboards with advanced filtering
- ğŸ¤– ML-powered ESG risk prediction (Low/Medium/High)
- ğŸ“‰ Trend analysis and historical comparisons

### Technical Excellence
- ğŸ”’ **Security**: Environment-based secrets, CORS, security headers, input validation
- ğŸ³ **Docker**: Full containerization with development and production configurations
- ğŸš€ **CI/CD**: Automated testing, linting, security scanning, and deployment
- ğŸ“¦ **MLflow**: Experiment tracking and model versioning
- ğŸ§ª **Testing**: Comprehensive unit and integration tests (backend & frontend)
- ğŸ“Š **Monitoring**: Structured logging and health checks
- âš¡ **Performance**: Code splitting, lazy loading, PWA support
- â™¿ **Accessibility**: React Error Boundaries and semantic HTML

## ğŸ“ Project Structure

```
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ dependabot.yml            # Automated dependency updates
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                # CI pipeline (test, lint, build)
â”‚       â””â”€â”€ deploy.yml            # Deployment automation
â”œâ”€â”€ backend/                      # FastAPI application
â”‚   â”œâ”€â”€ app.py                    # API with security middleware
â”‚   â”œâ”€â”€ db_config.py              # Database connection with env vars
â”‚   â”œâ”€â”€ logging_config.py         # Centralized logging
â”‚   â”œâ”€â”€ model.py                  # ML model inference
â”‚   â”œâ”€â”€ schemas.py                # Pydantic validation models
â”‚   â””â”€â”€ sql/                      # Database scripts
â”œâ”€â”€ frontend/                     # React + TypeScript + Vite
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ErrorBoundary.tsx # Error handling
â”‚   â”‚   â”‚   â””â”€â”€ ui/               # Shadcn UI components
â”‚   â”‚   â”œâ”€â”€ pages/                # Lazy-loaded routes
â”‚   â”‚   â””â”€â”€ test/                 # Test setup
â”‚   â”œâ”€â”€ Dockerfile                # Multi-stage build
â”‚   â”œâ”€â”€ nginx.conf                # Production web server config
â”‚   â””â”€â”€ vitest.config.ts          # Test configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ mlflow_config.py          # Experiment tracking
â”‚   â”œâ”€â”€ validate_model.py         # Model validation pipeline
â”‚   â””â”€â”€ train_pipeline.py         # Training automation
â”œâ”€â”€ tests/                        # Backend tests
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_schemas.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ docker-compose.yml            # Development environment
â”œâ”€â”€ docker-compose.prod.yml       # Production environment
â”œâ”€â”€ Dockerfile                    # Backend container
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ .pre-commit-config.yaml       # Code quality hooks
â”œâ”€â”€ pyproject.toml                # Python tooling config
â””â”€â”€ requirements.txt              # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Node.js 20+ / Bun
- PostgreSQL 15+
- Docker & Docker Compose (optional but recommended)

### Option 1: Docker (Recommended)

1. **Clone and configure**:
```bash
git clone https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis.git
cd ESG-Sustainability-Analysis
cp .env.example .env
# Edit .env with your settings
```

2. **Start services**:
```bash
docker-compose up -d
```

3. **Access applications**:
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/api/docs
- MLflow: http://localhost:5000

### Option 2: Local Development

1. **Backend setup**:
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your database credentials

# Run database migrations
psql -h localhost -U postgres -d esg_db -f backend/sql/schema.sql

# Start backend
python -m backend.app
```

2. **Frontend setup**:
```bash
cd frontend
bun install  # or: npm install
bun run dev  # or: npm run dev
```

## ğŸ§ª Testing

### Backend Tests
```bash
# Run all tests with coverage
pytest

# Run specific test file
pytest tests/test_api.py -v

# Run with coverage report
pytest --cov=backend --cov-report=html
```

### Frontend Tests
```bash
cd frontend

# Run tests
bun test

# Run with UI
bun test:ui

# Coverage report
bun test:coverage
```

### Security Scanning
```bash
# Check for vulnerabilities
safety check

# Security linting
bandit -r backend/ scripts/

# Run all pre-commit hooks
pre-commit run --all-files
```

## ğŸ“Š ML Model Training & Validation

### Train Model
```bash
python scripts/train_pipeline.py
```

### Validate Model
```bash
python scripts/validate_model.py \
  --model models/esg_risk_model.joblib \
  --test-data data/processed/test_data.csv \
  --output validation_report.txt
```

### Track Experiments with MLflow
```bash
# Start MLflow UI
mlflow ui --port 5000

# Training automatically logs to MLflow when configured
```

## ğŸš¢ Deployment

### Production Deployment with Docker
```bash
# Build and deploy production stack
docker-compose -f docker-compose.prod.yml up -d

# View logs
docker-compose -f docker-compose.prod.yml logs -f

# Scale services
docker-compose -f docker-compose.prod.yml up -d --scale backend=3
```

### Environment Variables for Production
```env
# .env.production
ENVIRONMENT=production
DEBUG=False
LOG_LEVEL=WARNING

# Database
DB_HOST=your-production-db-host
DB_NAME=esg_db_prod
DB_USER=prod_user
DB_PASSWORD=strong_password_here

# Security
SECRET_KEY=generate-with-openssl-rand-hex-32
ALLOWED_HOSTS=yourdomain.com,www.yourdomain.com
CORS_ORIGINS=https://yourdomain.com

# API
API_WORKERS=4
API_RELOAD=False

# Monitoring
SENTRY_DSN=your-sentry-dsn
```

### CI/CD Pipeline

The project includes automated GitHub Actions workflows:

**CI Pipeline** (`.github/workflows/ci.yml`):
- âœ… Runs on every push and PR
- ğŸ” Linting (Black, Flake8, ESLint)
- ğŸ”’ Security scanning (Bandit, Safety)
- ğŸ§ª Tests with coverage
- ğŸ³ Docker build validation

**Deploy Pipeline** (`.github/workflows/deploy.yml`):
- ğŸš€ Automated deployment to staging on main branch
- ğŸ“¦ Builds and pushes Docker images to GHCR
- ğŸ”„ Manual production deployment with approval
- â†©ï¸ Rollback capability

## ğŸ“– API Documentation

### Interactive API Docs
- Swagger UI: http://localhost:8000/api/docs
- ReDoc: http://localhost:8000/api/redoc

### Key Endpoints

#### Health Check
```http
GET /health
```

#### Analytics
```http
GET /companies/top?limit=10
GET /sectors/average
GET /companies/high-controversy?min_score=50
```

#### ML Predictions
```http
POST /predict
Content-Type: application/json

{
  "environment_risk_score": 45.0,
  "social_risk_score": 32.0,
  "governance_risk_score": 28.0,
  "controversy_score": 15.0,
  "full_time_employees": 50000
}
```

```http
POST /predict/batch
Content-Type: application/json

{
  "items": [
    { "environment_risk_score": 45.0, ... },
    { "environment_risk_score": 65.0, ... }
  ]
}
```

#### Model Management
```http
GET /model/info
GET /model/feature-importances
POST /model/reload
```

## ğŸ› ï¸ Development

### Pre-commit Hooks
```bash
# Install pre-commit hooks
pip install pre-commit
pre-commit install

# Run manually
pre-commit run --all-files
```

### Code Formatting
```bash
# Python
black backend/ scripts/
isort backend/ scripts/
flake8 backend/ scripts/

# TypeScript/React
cd frontend
bun run lint
bun run format  # If prettier script exists
```

### Database Migrations
```bash
# Create new migration
# (Add your migration tool here - Alembic recommended)

# Apply migrations
psql -h localhost -U postgres -d esg_db -f backend/sql/schema.sql
```

## ğŸ”’ Security Best Practices

This project implements multiple security layers:

1. **Environment Variables**: All secrets in `.env` files (never committed)
2. **Input Validation**: Pydantic schemas validate all API inputs
3. **Security Headers**: HSTS, X-Frame-Options, CSP, etc.
4. **CORS Configuration**: Restricted origins in production
5. **Dependency Scanning**: Automated with Dependabot
6. **Code Scanning**: Bandit for Python, ESLint for TypeScript
7. **Container Security**: Non-root users, minimal base images
8. **Database Security**: Parameterized queries, connection pooling

## ğŸ“ˆ Performance Optimizations

- **Frontend**:
  - Code splitting with React.lazy()
  - Image optimization and lazy loading
  - Service worker for offline support
  - CDN-ready static assets
  - Gzip compression

- **Backend**:
  - Database connection pooling
  - Query optimization and indexing
  - Response caching
  - Async request handling
  - Health check endpoints

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Contribution Guidelines
- Follow code style (Black, ESLint)
- Write tests for new features
- Update documentation
- Ensure CI passes

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- ESG data providers and sustainability reporting frameworks
- Open source community
- Contributors and maintainers

## ğŸ“§ Contact

**Surya Hariharan** - [@Surya-Hariharan](https://github.com/Surya-Hariharan)

Project Link: [https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis](https://github.com/Surya-Hariharan/ESG-Sustainability-Analysis)

---

**Built with â¤ï¸ for a sustainable future** ğŸŒ
4. (Optional) Load data via COPY (adjust path first) using `backend/sql/load_data.sql`.

## ğŸš€ Running the API
Install dependencies then start FastAPI with Uvicorn:
```
pip install -r requirements.txt
python -m uvicorn backend.app:app --reload --port 8000
```
Open: http://localhost:8000/docs for interactive Swagger.

## ğŸ¤– Training the Model
Prepare a processed CSV containing features:
Required columns: `environment_risk_score, social_risk_score, governance_risk_score, controversy_score, full_time_employees, esg_risk_level`
```
python scripts/train_model.py --input data/processed/esg_model_training.csv --output models/esg_risk_model.joblib
```
After training restart the API (it auto-loads the model at startup).

## ğŸ”® Making Predictions (Batch)
```
python scripts/predict.py --input data/processed/new_companies.csv --output reports/predictions/predictions.csv
```

Or via API:
```
POST /predict
{
	"environment_risk_score": 12.3,
	"social_risk_score": 18.4,
	"governance_risk_score": 10.2,
	"controversy_score": 25.0,
	"full_time_employees": 34000
}
```

## ğŸ“Š Key Endpoints
| Endpoint | Description |
|----------|-------------|
| `/health` | Health & readiness (DB + model) |
| `/companies/top?limit=10` | Lowest ESG risk companies |
| `/sectors/average` | Sector-level average scores |
| `/companies/high-controversy?min_score=50` | High controversy filter |
| `/predict` | Single prediction |
| `/predict/batch` | Batch predictions |

## âœ… Data Quality & Reproducibility
Notebooks provide a transparent lineage from raw data â†’ cleaned â†’ modeling dataset. Scripts operationalize those flows for automation / scheduling.

## ğŸ§ª Testing (Suggested Additions)
Add unit tests for:
- DB utility functions (mocked connection)
- Model prediction shape
- API endpoint responses (TestClient)

## ğŸ” Security Notes
- Restrict `allow_origins` in production.
- Use role-based DB credentials with least privilege.
- Consider adding rate limiting & API key auth for hosted deployment.

## ğŸ“Œ Roadmap Ideas
- Add CI workflow (lint + tests + Docker build)
- Feature importance & SHAP analysis endpoint
- Incremental data ingestion pipeline (Airflow / Prefect)
- Dashboard container (Dash or Streamlit) with reverse proxy

## ğŸ“ License
MIT License. See `LICENSE` for details.

---
If you find this useful, a star â­ on the repository helps others discover it.

