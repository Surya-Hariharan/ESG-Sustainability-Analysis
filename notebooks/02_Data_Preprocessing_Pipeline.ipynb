{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2640fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Data preprocessing started on: 2026-01-26 14:55:58\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Data preprocessing started on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5e0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Raw dataset loaded successfully!\n",
      "üìä Original dataset shape: 503 rows √ó 15 columns\n",
      "üíæ Memory usage: 1.04 MB\n",
      "\n",
      "üîÑ Working copy created for preprocessing\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/raw/dataset.csv'\n",
    "output_path = '../data/processed/'\n",
    "try:\n",
    "    df_raw = pd.read_csv(data_path)\n",
    "    print(\"‚úÖ Raw dataset loaded successfully!\")\n",
    "    print(f\"üìä Original dataset shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")\n",
    "    print(f\"üíæ Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset file not found. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "df = df_raw.copy()\n",
    "print(f\"\\nüîÑ Working copy created for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726587d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INITIAL DATA ASSESSMENT\n",
      "==================================================\n",
      "Dataset Shape: (503, 15)\n",
      "\n",
      "Column Information:\n",
      " 1. Symbol                    | Missing:   0 (  0.0%)\n",
      " 2. Name                      | Missing:   0 (  0.0%)\n",
      " 3. Address                   | Missing:   1 (  0.2%)\n",
      " 4. Sector                    | Missing:   1 (  0.2%)\n",
      " 5. Industry                  | Missing:   1 (  0.2%)\n",
      " 6. Full Time Employees       | Missing:   5 (  1.0%)\n",
      " 7. Description               | Missing:   1 (  0.2%)\n",
      " 8. Total ESG Risk score      | Missing:  73 ( 14.5%)\n",
      " 9. Environment Risk Score    | Missing:  73 ( 14.5%)\n",
      "10. Governance Risk Score     | Missing:  73 ( 14.5%)\n",
      "11. Social Risk Score         | Missing:  73 ( 14.5%)\n",
      "12. Controversy Level         | Missing:  73 ( 14.5%)\n",
      "13. Controversy Score         | Missing: 100 ( 19.9%)\n",
      "14. ESG Risk Percentile       | Missing:  73 ( 14.5%)\n",
      "15. ESG Risk Level            | Missing:  73 ( 14.5%)\n",
      "\n",
      "üìä Total Missing Values: 620\n",
      "üìä Data Completeness: 91.8%\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç INITIAL DATA ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Information:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {col:<25} | Missing: {missing_count:3d} ({missing_pct:5.1f}%)\")\n",
    "print(f\"\\nüìä Total Missing Values: {df.isnull().sum().sum():,}\")\n",
    "print(f\"üìä Data Completeness: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f515195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STEP 1: BASIC DATA CLEANING\n",
      "========================================\n",
      "Duplicate rows found: 0\n",
      "\n",
      "üìã Company Symbol Analysis:\n",
      "Duplicate symbols: 0\n",
      "‚úÖ All company symbols are unique\n",
      "\n",
      "üìä Shape after basic cleaning: (503, 15) (removed 0 rows)\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ STEP 1: BASIC DATA CLEANING\")\n",
    "print(\"=\" * 40)\n",
    "initial_shape = df.shape\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {duplicate_rows}\")\n",
    "if duplicate_rows > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"‚úÖ Removed {duplicate_rows} duplicate rows\")\n",
    "print(f\"\\nüìã Company Symbol Analysis:\")\n",
    "if 'Symbol' in df.columns:\n",
    "    duplicate_symbols = df['Symbol'].duplicated().sum()\n",
    "    print(f\"Duplicate symbols: {duplicate_symbols}\")\n",
    "    if duplicate_symbols > 0:\n",
    "        print(\"üîç Duplicate symbols found:\")\n",
    "        duplicated_syms = df[df['Symbol'].duplicated(keep=False)]['Symbol'].value_counts()\n",
    "        print(duplicated_syms)\n",
    "    else:\n",
    "        print(\"‚úÖ All company symbols are unique\")\n",
    "print(f\"\\nüìä Shape after basic cleaning: {df.shape} (removed {initial_shape[0] - df.shape[0]} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbab5bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STEP 2: EMPLOYEE DATA STANDARDIZATION\n",
      "=============================================\n",
      "Processing employee count data...\n",
      "Original data type: object\n",
      "‚úÖ Employee data standardized:\n",
      "   Valid records: 498/503\n",
      "   Range: 28 to 2,100,000\n",
      "   Median: 21,585\n",
      "\n",
      "‚ö†Ô∏è  Large companies (>1M employees):\n",
      "   WMT: 2,100,000 employees\n",
      "   AMZN: 1,525,000 employees\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ STEP 2: EMPLOYEE DATA STANDARDIZATION\")\n",
    "print(\"=\" * 45)\n",
    "if 'Full Time Employees' in df.columns:\n",
    "    print(\"Processing employee count data...\")\n",
    "    print(f\"Original data type: {df['Full Time Employees'].dtype}\")\n",
    "    \n",
    "    def clean_employee_count(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        \n",
    "        value_str = str(value).strip()\n",
    "        if value_str == '' or value_str.lower() in ['nan', 'null', 'none']:\n",
    "            return np.nan\n",
    "        \n",
    "        value_clean = re.sub(r'[^0-9.]', '', value_str)\n",
    "        try:\n",
    "            return float(value_clean)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    df['Full Time Employees'] = df['Full Time Employees'].apply(clean_employee_count)\n",
    "    \n",
    "    employee_stats = df['Full Time Employees'].describe()\n",
    "    print(f\"‚úÖ Employee data standardized:\")\n",
    "    print(f\"   Valid records: {df['Full Time Employees'].count()}/{len(df)}\")\n",
    "    print(f\"   Range: {employee_stats['min']:,.0f} to {employee_stats['max']:,.0f}\")\n",
    "    print(f\"   Median: {employee_stats['50%']:,.0f}\")\n",
    "    \n",
    "    extreme_outliers = df['Full Time Employees'] > 1000000\n",
    "    if extreme_outliers.any():\n",
    "        print(f\"\\n‚ö†Ô∏è  Large companies (>1M employees):\")\n",
    "        large_companies = df[extreme_outliers][['Symbol', 'Name', 'Full Time Employees']]\n",
    "        for _, row in large_companies.iterrows():\n",
    "            print(f\"   {row['Symbol']}: {row['Full Time Employees']:,.0f} employees\")\n",
    "else:\n",
    "    print(\"‚ùå 'Full Time Employees' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb06964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STEP 3: ESG RISK SCORES PREPROCESSING\n",
      "=============================================\n",
      "üéØ Identified 5 ESG-related numeric columns:\n",
      "   1. Total ESG Risk score\n",
      "   2. Environment Risk Score\n",
      "   3. Governance Risk Score\n",
      "   4. Social Risk Score\n",
      "   5. Controversy Score\n",
      "\n",
      "üìä ESG Data Completeness Analysis:\n",
      "   Total ESG Risk score     : 430/503 valid ( 85.5%)\n",
      "   Environment Risk Score   : 430/503 valid ( 85.5%)\n",
      "   Governance Risk Score    : 430/503 valid ( 85.5%)\n",
      "   Social Risk Score        : 430/503 valid ( 85.5%)\n",
      "   Controversy Score        : 403/503 valid ( 80.1%)\n",
      "\n",
      "üîç ESG Score Ranges:\n",
      "   Total ESG Risk score     :    7.1 -   41.7 (mean:   21.5)\n",
      "   Environment Risk Score   :    0.0 -   25.0 (mean:    5.7)\n",
      "   Governance Risk Score    :    3.0 -   19.4 (mean:    6.7)\n",
      "   Social Risk Score        :    0.8 -   22.5 (mean:    9.1)\n",
      "   Controversy Score        :    1.0 -    5.0 (mean:    2.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ STEP 3: ESG RISK SCORES PREPROCESSING\")\n",
    "print(\"=\" * 45)\n",
    "esg_columns = []\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['esg', 'risk', 'score', 'environment', 'social', 'governance']):\n",
    "        if df[col].dtype in ['int64', 'float64'] or col in ['Total ESG Risk score', 'Environment Risk Score', 'Governance Risk Score', 'Social Risk Score', 'Controversy Score']:\n",
    "            esg_columns.append(col)\n",
    "print(f\"üéØ Identified {len(esg_columns)} ESG-related numeric columns:\")\n",
    "for i, col in enumerate(esg_columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "if esg_columns:\n",
    "    print(f\"\\nüìä ESG Data Completeness Analysis:\")\n",
    "    for col in esg_columns:\n",
    "        valid_count = df[col].count()\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        print(f\"   {col:<25}: {valid_count:3d}/{len(df)} valid ({100-missing_pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîç ESG Score Ranges:\")\n",
    "    for col in esg_columns:\n",
    "        if df[col].count() > 0:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            mean_val = df[col].mean()\n",
    "            print(f\"   {col:<25}: {min_val:6.1f} - {max_val:6.1f} (mean: {mean_val:6.1f})\")\n",
    "else:\n",
    "    print(\"‚ùå No ESG columns identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e73d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STEP 4: CATEGORICAL DATA STANDARDIZATION\n",
      "=============================================\n",
      "\n",
      "üìä Processing Sector:\n",
      "   Unique values: 12 (including NaN)\n",
      "   Missing: 1 (0.2%)\n",
      "   Top 5 categories: ['Technology', 'Industrials', 'Financial Services', 'Healthcare', 'Consumer Cyclical']\n",
      "\n",
      "üìä Processing Industry:\n",
      "   Unique values: 117 (including NaN)\n",
      "   Missing: 1 (0.2%)\n",
      "   Top 5 categories: ['Utilities - Regulated Electric', 'Specialty Industrial Machinery', 'Semiconductors', 'Software - Application', 'Aerospace & Defense']\n",
      "\n",
      "üìä Processing Controversy Level:\n",
      "   Unique values: 7 (including NaN)\n",
      "   Missing: 73 (14.5%)\n",
      "   Top categories: ['Moderate Controversy Level', 'Low Controversy Level', 'Significant Controversy Level', nan, 'None Controversy Level']\n",
      "\n",
      "üìä Processing ESG Risk Level:\n",
      "   Unique values: 6 (including NaN)\n",
      "   Missing: 73 (14.5%)\n",
      "   Top categories: ['Low', 'Medium', nan, 'High', 'Negligible']\n",
      "\n",
      "üìä Processing ESG Risk Percentile:\n",
      "   Unique values: 90 (including NaN)\n",
      "   Missing: 73 (14.5%)\n",
      "   Sample values: [nan, '16th percentile', '23rd percentile', '9th percentile', '6th percentile']\n",
      "   ‚úÖ Created numeric percentile column: 1-93 range\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ STEP 4: CATEGORICAL DATA STANDARDIZATION\")\n",
    "print(\"=\" * 45)\n",
    "categorical_columns = ['Sector', 'Industry', 'Controversy Level', 'ESG Risk Level', 'ESG Risk Percentile']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nüìä Processing {col}:\")\n",
    "        \n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace(['nan', 'None', 'null', ''], np.nan)\n",
    "        \n",
    "        unique_values = df[col].value_counts(dropna=False)\n",
    "        print(f\"   Unique values: {len(unique_values)} (including NaN)\")\n",
    "        print(f\"   Missing: {df[col].isnull().sum()} ({(df[col].isnull().sum()/len(df)*100):.1f}%)\")\n",
    "        \n",
    "        if col == 'ESG Risk Percentile':\n",
    "            print(f\"   Sample values: {list(unique_values.head().index)}\")\n",
    "            \n",
    "            def extract_percentile_number(value):\n",
    "                if pd.isna(value) or value == 'nan':\n",
    "                    return np.nan\n",
    "                try:\n",
    "                    percentile_match = re.search(r'(\\d+)', str(value))\n",
    "                    if percentile_match:\n",
    "                        return int(percentile_match.group(1))\n",
    "                    return np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "            \n",
    "            df['ESG_Risk_Percentile_Numeric'] = df[col].apply(extract_percentile_number)\n",
    "            valid_percentiles = df['ESG_Risk_Percentile_Numeric'].dropna()\n",
    "            if len(valid_percentiles) > 0:\n",
    "                print(f\"   ‚úÖ Created numeric percentile column: {valid_percentiles.min():.0f}-{valid_percentiles.max():.0f} range\")\n",
    "        \n",
    "        elif col in ['Controversy Level', 'ESG Risk Level']:\n",
    "            print(f\"   Top categories: {list(unique_values.head().index)}\")\n",
    "        \n",
    "        elif col in ['Sector', 'Industry']:\n",
    "            print(f\"   Top 5 categories: {list(unique_values.head().index)}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {col} not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81be6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STEP 5: MISSING VALUE TREATMENT STRATEGY\n",
      "==================================================\n",
      "üìä Missing Value Categories:\n",
      "   üî¥ High missing (>10%): 9 columns\n",
      "   üü° Medium missing (1-10%): 0 columns\n",
      "   üü¢ Low missing (<1%): 5 columns\n",
      "\n",
      "üî¥ High Missing Columns:\n",
      "   Controversy Score        :  19.9% missing\n",
      "   Total ESG Risk score     :  14.5% missing\n",
      "   Environment Risk Score   :  14.5% missing\n",
      "   Governance Risk Score    :  14.5% missing\n",
      "   Social Risk Score        :  14.5% missing\n",
      "   Controversy Level        :  14.5% missing\n",
      "   ESG Risk Percentile      :  14.5% missing\n",
      "   ESG Risk Level           :  14.5% missing\n",
      "   ESG_Risk_Percentile_Numeric:  14.5% missing\n",
      "\n",
      "üìã Imputation Strategy Summary:\n",
      "   forward_fill: 5 columns\n",
      "   knn_imputation: 5 columns\n",
      "   median_mode: 4 columns\n",
      "   no_action: 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ STEP 5: MISSING VALUE TREATMENT STRATEGY\")\n",
    "print(\"=\" * 50)\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Data_Type': df.dtypes\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "high_missing = missing_analysis[missing_analysis['Missing_Percentage'] > 10]\n",
    "medium_missing = missing_analysis[(missing_analysis['Missing_Percentage'] > 1) & (missing_analysis['Missing_Percentage'] <= 10)]\n",
    "low_missing = missing_analysis[(missing_analysis['Missing_Percentage'] > 0) & (missing_analysis['Missing_Percentage'] <= 1)]\n",
    "print(f\"üìä Missing Value Categories:\")\n",
    "print(f\"   üî¥ High missing (>10%): {len(high_missing)} columns\")\n",
    "print(f\"   üü° Medium missing (1-10%): {len(medium_missing)} columns\")\n",
    "print(f\"   üü¢ Low missing (<1%): {len(low_missing)} columns\")\n",
    "if len(high_missing) > 0:\n",
    "    print(f\"\\nüî¥ High Missing Columns:\")\n",
    "    for _, row in high_missing.iterrows():\n",
    "        print(f\"   {row['Column']:<25}: {row['Missing_Percentage']:5.1f}% missing\")\n",
    "imputation_strategy = {}\n",
    "for col in df.columns:\n",
    "    missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    if missing_pct == 0:\n",
    "        imputation_strategy[col] = 'no_action'\n",
    "    elif missing_pct > 25:\n",
    "        imputation_strategy[col] = 'flag_and_median'  # Flag missing + impute with median/mode\n",
    "    elif missing_pct > 10:\n",
    "        if col in esg_columns:\n",
    "            imputation_strategy[col] = 'knn_imputation'  # Use KNN for ESG scores\n",
    "        else:\n",
    "            imputation_strategy[col] = 'median_mode'\n",
    "    elif missing_pct > 1:\n",
    "        imputation_strategy[col] = 'median_mode'\n",
    "    else:\n",
    "        imputation_strategy[col] = 'forward_fill'\n",
    "print(f\"\\nüìã Imputation Strategy Summary:\")\n",
    "strategy_counts = pd.Series(list(imputation_strategy.values())).value_counts()\n",
    "for strategy, count in strategy_counts.items():\n",
    "    print(f\"   {strategy}: {count} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd4c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß STEP 6: IMPLEMENTING MISSING VALUE IMPUTATION\n",
      "==================================================\n",
      "   Address                       :   1 ‚Üí   0 missing (forward_fill)\n",
      "   Sector                        :   1 ‚Üí   0 missing (forward_fill)\n",
      "   Industry                      :   1 ‚Üí   0 missing (forward_fill)\n",
      "   Full Time Employees           :   5 ‚Üí   0 missing (forward_fill)\n",
      "   Description                   :   1 ‚Üí   0 missing (forward_fill)\n",
      "   Total ESG Risk score          :  73 ‚Üí   0 missing (knn_imputation)\n",
      "   Environment Risk Score        :  73 ‚Üí   0 missing (knn_imputation)\n",
      "   Governance Risk Score         :  73 ‚Üí   0 missing (knn_imputation)\n",
      "   Social Risk Score             :  73 ‚Üí   0 missing (knn_imputation)\n",
      "   Controversy Level             :  73 ‚Üí   0 missing (median_mode)\n",
      "   Controversy Score             : 100 ‚Üí   0 missing (knn_imputation)\n",
      "   ESG Risk Percentile           :  73 ‚Üí   0 missing (median_mode)\n",
      "   ESG Risk Level                :  73 ‚Üí   0 missing (median_mode)\n",
      "   ESG_Risk_Percentile_Numeric   :  73 ‚Üí   0 missing (median_mode)\n",
      "\n",
      "‚úÖ Imputation completed!\n",
      "üìä Total missing values: 693 ‚Üí 0\n",
      "üìä Data completeness: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß STEP 6: IMPLEMENTING MISSING VALUE IMPUTATION\")\n",
    "print(\"=\" * 50)\n",
    "df_imputed = df.copy()\n",
    "for col, strategy in imputation_strategy.items():\n",
    "    if strategy == 'no_action':\n",
    "        continue\n",
    "    \n",
    "    missing_before = df_imputed[col].isnull().sum()\n",
    "    \n",
    "    if strategy == 'forward_fill':\n",
    "        if df_imputed[col].dtype in ['object']:\n",
    "            df_imputed[col].fillna(method='ffill', inplace=True)\n",
    "            df_imputed[col].fillna('Unknown', inplace=True)\n",
    "        else:\n",
    "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    elif strategy == 'median_mode':\n",
    "        if df_imputed[col].dtype in ['object']:\n",
    "            mode_val = df_imputed[col].mode()\n",
    "            fill_val = mode_val.iloc[0] if len(mode_val) > 0 else 'Unknown'\n",
    "            df_imputed[col].fillna(fill_val, inplace=True)\n",
    "        else:\n",
    "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    elif strategy == 'flag_and_median':\n",
    "        df_imputed[f'{col}_was_missing'] = df_imputed[col].isnull().astype(int)\n",
    "        if df_imputed[col].dtype in ['object']:\n",
    "            mode_val = df_imputed[col].mode()\n",
    "            fill_val = mode_val.iloc[0] if len(mode_val) > 0 else 'Unknown'\n",
    "            df_imputed[col].fillna(fill_val, inplace=True)\n",
    "        else:\n",
    "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    elif strategy == 'knn_imputation' and col in esg_columns:\n",
    "        if df_imputed[col].count() > 0:  # Only if there are some non-null values\n",
    "            # Create a subset for KNN imputation\n",
    "            esg_subset = df_imputed[esg_columns].select_dtypes(include=[np.number])\n",
    "            if esg_subset.shape[1] > 1:  # Need at least 2 columns for KNN\n",
    "                try:\n",
    "                    imputer = KNNImputer(n_neighbors=5)\n",
    "                    esg_imputed = imputer.fit_transform(esg_subset)\n",
    "                    \n",
    "                    # Update only the current column\n",
    "                    col_idx = esg_subset.columns.get_loc(col)\n",
    "                    df_imputed[col] = esg_imputed[:, col_idx]\n",
    "                except:\n",
    "                    # Fallback to median if KNN fails\n",
    "                    df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "            else:\n",
    "                df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    missing_after = df_imputed[col].isnull().sum()\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        print(f\"   {col:<30}: {missing_before:3d} ‚Üí {missing_after:3d} missing ({strategy})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Imputation completed!\")\n",
    "print(f\"üìä Total missing values: {df.isnull().sum().sum()} ‚Üí {df_imputed.isnull().sum().sum()}\")\n",
    "print(f\"üìä Data completeness: {((df_imputed.shape[0] * df_imputed.shape[1] - df_imputed.isnull().sum().sum()) / (df_imputed.shape[0] * df_imputed.shape[1]) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ffefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß STEP 7: OUTLIER DETECTION AND TREATMENT\n",
      "=============================================\n",
      "üìä Outlier Analysis:\n",
      "   Controversy Score             : 218 outliers (43.3%)\n",
      "   Full Time Employees           :  52 outliers (10.3%)\n",
      "   Governance Risk Score         :  19 outliers ( 3.8%)\n",
      "   Social Risk Score             :  18 outliers ( 3.6%)\n",
      "   Environment Risk Score        :   9 outliers ( 1.8%)\n",
      "   Total ESG Risk score          :   7 outliers ( 1.4%)\n",
      "\n",
      "üéØ Outlier Treatment Strategy:\n",
      "   Full Time Employees: Capped 6 values at 99th percentile (439,500)\n",
      "   Controversy Score: Keeping 218 outliers (legitimate ESG risk variation)\n",
      "\n",
      "‚úÖ Outlier treatment completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß STEP 7: OUTLIER DETECTION AND TREATMENT\")\n",
    "print(\"=\" * 45)\n",
    "numeric_columns = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = []\n",
    "for col in numeric_columns:\n",
    "    if df_imputed[col].count() > 0:\n",
    "        Q1 = df_imputed[col].quantile(0.25)\n",
    "        Q3 = df_imputed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df_imputed[(df_imputed[col] < lower_bound) | (df_imputed[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df_imputed)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outliers': outlier_count,\n",
    "            'Percentage': outlier_percentage,\n",
    "            'Lower_Bound': lower_bound,\n",
    "            'Upper_Bound': upper_bound\n",
    "        })\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df.sort_values('Percentage', ascending=False)\n",
    "print(\"üìä Outlier Analysis:\")\n",
    "for _, row in outlier_df.head(10).iterrows():\n",
    "    if row['Outliers'] > 0:\n",
    "        print(f\"   {row['Column']:<30}: {row['Outliers']:3d} outliers ({row['Percentage']:4.1f}%)\")\n",
    "print(f\"\\nüéØ Outlier Treatment Strategy:\")\n",
    "for col in numeric_columns:\n",
    "    if col in outlier_df[outlier_df['Percentage'] > 5]['Column'].values:\n",
    "        if 'Employee' in col:\n",
    "            # Cap employee outliers at 99th percentile\n",
    "            cap_value = df_imputed[col].quantile(0.99)\n",
    "            outlier_count = (df_imputed[col] > cap_value).sum()\n",
    "            df_imputed[col] = np.where(df_imputed[col] > cap_value, cap_value, df_imputed[col])\n",
    "            print(f\"   {col}: Capped {outlier_count} values at 99th percentile ({cap_value:,.0f})\")\n",
    "        elif col in esg_columns:\n",
    "            # For ESG scores, use IQR method but don't cap (they might be legitimate)\n",
    "            outlier_count = len(outlier_df[outlier_df['Column'] == col])\n",
    "            if outlier_count > 0:\n",
    "                print(f\"   {col}: Keeping {outlier_df[outlier_df['Column']==col]['Outliers'].iloc[0]} outliers (legitimate ESG risk variation)\")\n",
    "        else:\n",
    "            # For other numeric columns, flag outliers\n",
    "            outlier_info = outlier_df[outlier_df['Column'] == col].iloc[0]\n",
    "            if outlier_info['Outliers'] > 0:\n",
    "                print(f\"   {col}: Flagged {outlier_info['Outliers']} outliers for review\")\n",
    "\n",
    "print(f\"\\n‚úÖ Outlier treatment completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23869d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß STEP 8: FEATURE ENGINEERING\n",
      "===================================\n",
      "‚úÖ Created ESG_Component_Balance (std of E, S, G scores)\n",
      "‚úÖ Created ESG_Max_Component and ESG_Min_Component\n",
      "‚úÖ Created Employee_Size_Category (Small/Medium/Large/Enterprise)\n",
      "‚úÖ Created Log_Employees (log-transformed employee count)\n",
      "‚úÖ Created ESG_Risk_Above_Average (threshold: 21.5)\n",
      "‚úÖ Created ESG_Risk_Category (Low/Medium/High/Severe)\n",
      "‚úÖ Created Sector_Risk_Average and ESG_vs_Sector_Average\n",
      "‚úÖ Created High_Risk_Percentile (>75th percentile flag)\n",
      "\n",
      "üìä Feature Engineering Summary:\n",
      "   Original features: 16\n",
      "   New features: 10\n",
      "   Total features: 26\n",
      "\n",
      "üÜï New features created:\n",
      "   1. ESG_Component_Balance\n",
      "   2. ESG_Max_Component\n",
      "   3. ESG_Min_Component\n",
      "   4. Employee_Size_Category\n",
      "   5. Log_Employees\n",
      "   6. ESG_Risk_Above_Average\n",
      "   7. ESG_Risk_Category\n",
      "   8. Sector_Risk_Average\n",
      "   9. ESG_vs_Sector_Average\n",
      "   10. High_Risk_Percentile\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß STEP 8: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 35)\n",
    "df_featured = df_imputed.copy()\n",
    "if len([col for col in esg_columns if col in df_featured.columns]) >= 3:\n",
    "    if all(col in df_featured.columns for col in ['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']):\n",
    "        df_featured['ESG_Component_Balance'] = df_featured[['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']].std(axis=1)\n",
    "        print(\"‚úÖ Created ESG_Component_Balance (std of E, S, G scores)\")\n",
    "        \n",
    "        df_featured['ESG_Max_Component'] = df_featured[['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']].max(axis=1)\n",
    "        df_featured['ESG_Min_Component'] = df_featured[['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']].min(axis=1)\n",
    "        print(\"‚úÖ Created ESG_Max_Component and ESG_Min_Component\")\n",
    "if 'Full Time Employees' in df_featured.columns:\n",
    "    df_featured['Employee_Size_Category'] = pd.cut(\n",
    "        df_featured['Full Time Employees'], \n",
    "        bins=[0, 1000, 10000, 50000, float('inf')], \n",
    "        labels=['Small', 'Medium', 'Large', 'Enterprise'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    print(\"‚úÖ Created Employee_Size_Category (Small/Medium/Large/Enterprise)\")\n",
    "    \n",
    "    df_featured['Log_Employees'] = np.log1p(df_featured['Full Time Employees'])\n",
    "    print(\"‚úÖ Created Log_Employees (log-transformed employee count)\")\n",
    "if 'Total ESG Risk score' in df_featured.columns:\n",
    "    esg_risk_mean = df_featured['Total ESG Risk score'].mean()\n",
    "    df_featured['ESG_Risk_Above_Average'] = (df_featured['Total ESG Risk score'] > esg_risk_mean).astype(int)\n",
    "    print(f\"‚úÖ Created ESG_Risk_Above_Average (threshold: {esg_risk_mean:.1f})\")\n",
    "    \n",
    "    df_featured['ESG_Risk_Category'] = pd.cut(\n",
    "        df_featured['Total ESG Risk score'],\n",
    "        bins=[0, 15, 25, 35, float('inf')],\n",
    "        labels=['Low', 'Medium', 'High', 'Severe'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    print(\"‚úÖ Created ESG_Risk_Category (Low/Medium/High/Severe)\")\n",
    "if 'Sector' in df_featured.columns:\n",
    "    sector_risk_avg = df_featured.groupby('Sector')['Total ESG Risk score'].mean()\n",
    "    df_featured['Sector_Risk_Average'] = df_featured['Sector'].map(sector_risk_avg)\n",
    "    df_featured['ESG_vs_Sector_Average'] = df_featured['Total ESG Risk score'] - df_featured['Sector_Risk_Average']\n",
    "    print(\"‚úÖ Created Sector_Risk_Average and ESG_vs_Sector_Average\")\n",
    "if 'ESG Risk Percentile' in df_featured.columns and 'ESG_Risk_Percentile_Numeric' in df_featured.columns:\n",
    "    df_featured['High_Risk_Percentile'] = (df_featured['ESG_Risk_Percentile_Numeric'] > 75).astype(int)\n",
    "    print(\"‚úÖ Created High_Risk_Percentile (>75th percentile flag)\")\n",
    "new_features = [col for col in df_featured.columns if col not in df_imputed.columns]\n",
    "print(f\"\\nüìä Feature Engineering Summary:\")\n",
    "print(f\"   Original features: {len(df_imputed.columns)}\")\n",
    "print(f\"   New features: {len(new_features)}\")\n",
    "print(f\"   Total features: {len(df_featured.columns)}\")\n",
    "if new_features:\n",
    "    print(f\"\\nüÜï New features created:\")\n",
    "    for i, feature in enumerate(new_features, 1):\n",
    "        print(f\"   {i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ae99b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß STEP 9: ENCODING CATEGORICAL VARIABLES\n",
      "=============================================\n",
      "üìä Categorical columns to encode: 7\n",
      "\n",
      "üî§ Processing Sector:\n",
      "   Unique values: 11\n",
      "   ‚úÖ Label encoded as Sector_Encoded\n",
      "   üìù Keeping original Sector for reference\n",
      "\n",
      "üî§ Processing Industry:\n",
      "   Unique values: 116\n",
      "   ‚úÖ Label encoded as Industry_Encoded\n",
      "   üìù Keeping original Industry for reference\n",
      "\n",
      "üî§ Processing Controversy Level:\n",
      "   Unique values: 6\n",
      "   ‚úÖ One-hot encoded into 7 columns\n",
      "   üìù Keeping original Controversy Level for reference\n",
      "\n",
      "üî§ Processing ESG Risk Percentile:\n",
      "   Unique values: 89\n",
      "   ‚úÖ Label encoded as ESG Risk Percentile_Encoded\n",
      "   üìù Keeping original ESG Risk Percentile for reference\n",
      "\n",
      "üî§ Processing ESG Risk Level:\n",
      "   Unique values: 5\n",
      "   ‚úÖ One-hot encoded into 6 columns\n",
      "   üìù Keeping original ESG Risk Level for reference\n",
      "\n",
      "üî§ Processing Employee_Size_Category:\n",
      "   Unique values: 4\n",
      "   ‚úÖ One-hot encoded into 5 columns\n",
      "   üìù Keeping original Employee_Size_Category for reference\n",
      "\n",
      "üî§ Processing ESG_Risk_Category:\n",
      "   Unique values: 4\n",
      "   ‚úÖ One-hot encoded into 5 columns\n",
      "   üìù Keeping original ESG_Risk_Category for reference\n",
      "\n",
      "üìä Encoding Summary:\n",
      "   Columns after encoding: 52\n",
      "   New encoded columns: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß STEP 9: ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\" * 45)\n",
    "df_encoded = df_featured.copy()\n",
    "categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['Symbol', 'Name', 'Address', 'Description']]\n",
    "print(f\"üìä Categorical columns to encode: {len(categorical_cols)}\")\n",
    "if len(categorical_cols) > 0:\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nüî§ Processing {col}:\")\n",
    "        unique_count = df_encoded[col].nunique()\n",
    "        print(f\"   Unique values: {unique_count}\")\n",
    "        \n",
    "        if unique_count <= 10:  # One-hot encode low cardinality\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=f'{col}', dummy_na=True)\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            print(f\"   ‚úÖ One-hot encoded into {len(dummies.columns)} columns\")\n",
    "        else:  # Label encode high cardinality\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[f'{col}_Encoded'] = le.fit_transform(df_encoded[col].fillna('Unknown'))\n",
    "            print(f\"   ‚úÖ Label encoded as {col}_Encoded\")\n",
    "        \n",
    "        # Keep original for reference but mark for potential removal\n",
    "        print(f\"   üìù Keeping original {col} for reference\")\n",
    "print(f\"\\nüìä Encoding Summary:\")\n",
    "print(f\"   Columns after encoding: {len(df_encoded.columns)}\")\n",
    "new_encoded_cols = [col for col in df_encoded.columns if col not in df_featured.columns]\n",
    "print(f\"   New encoded columns: {len(new_encoded_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9abf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß STEP 10: FEATURE SCALING AND NORMALIZATION\n",
      "==================================================\n",
      "üìä Columns to scale: 24\n",
      "   ESG-related: ['Total ESG Risk score', 'Environment Risk Score', 'Governance Risk Score', 'Social Risk Score', 'Controversy Score', 'ESG_Risk_Percentile_Numeric', 'ESG_Component_Balance', 'ESG_Max_Component', 'ESG_Min_Component', 'ESG_Risk_Above_Average', 'ESG_vs_Sector_Average', 'High_Risk_Percentile', 'ESG_Risk_Category_Low', 'ESG_Risk_Category_Medium', 'ESG_Risk_Category_High', 'ESG_Risk_Category_Severe', 'ESG_Risk_Category_nan']\n",
      "   Other numeric: ['Full Time Employees', 'Log_Employees', 'Employee_Size_Category_Small', 'Employee_Size_Category_Medium', 'Employee_Size_Category_Large', 'Employee_Size_Category_Enterprise', 'Employee_Size_Category_nan']\n",
      "\n",
      "‚úÖ Standardized 24 numeric columns (mean=0, std=1)\n",
      "\n",
      "üìä Post-scaling statistics:\n",
      "   Mean range: -0.000 to 0.000\n",
      "   Std range: 0.000 to 1.001\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß STEP 10: FEATURE SCALING AND NORMALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "df_final = df_encoded.copy()\n",
    "numeric_cols_to_scale = df_final.select_dtypes(include=[np.number]).columns\n",
    "id_cols = ['Symbol', 'Name', 'Address', 'Description']\n",
    "exclude_from_scaling = ['Symbol', 'Name', 'Address', 'Description'] + \\\n",
    "                      [col for col in df_final.columns if '_was_missing' in col or \n",
    "                       col.endswith('_Encoded') or \n",
    "                       col.startswith(tuple(['Sector_', 'Industry_', 'Controversy Level_', 'ESG Risk Level_']))]\n",
    "scale_cols = [col for col in numeric_cols_to_scale if col not in exclude_from_scaling]\n",
    "print(f\"üìä Columns to scale: {len(scale_cols)}\")\n",
    "print(f\"   ESG-related: {[col for col in scale_cols if any(kw in col.lower() for kw in ['esg', 'risk', 'score', 'environment', 'social', 'governance'])]}\")\n",
    "print(f\"   Other numeric: {[col for col in scale_cols if not any(kw in col.lower() for kw in ['esg', 'risk', 'score', 'environment', 'social', 'governance'])]}\")\n",
    "if len(scale_cols) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    df_final[scale_cols] = scaler.fit_transform(df_final[scale_cols])\n",
    "    print(f\"\\n‚úÖ Standardized {len(scale_cols)} numeric columns (mean=0, std=1)\")\n",
    "    \n",
    "    scaling_stats = pd.DataFrame({\n",
    "        'Mean': df_final[scale_cols].mean(),\n",
    "        'Std': df_final[scale_cols].std(),\n",
    "        'Min': df_final[scale_cols].min(),\n",
    "        'Max': df_final[scale_cols].max()\n",
    "    })\n",
    "    print(f\"\\nüìä Post-scaling statistics:\")\n",
    "    print(f\"   Mean range: {scaling_stats['Mean'].min():.3f} to {scaling_stats['Mean'].max():.3f}\")\n",
    "    print(f\"   Std range: {scaling_stats['Std'].min():.3f} to {scaling_stats['Std'].max():.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå No columns found for scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362a9654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FINAL DATA QUALITY ASSESSMENT\n",
      "=============================================\n",
      "üéØ Processing Summary:\n",
      "   Original shape: (503, 15)\n",
      "   Final shape: (503, 52)\n",
      "   Rows added/removed: +0\n",
      "   Columns added: +37\n",
      "\n",
      "üìà Data Quality Metrics:\n",
      "   Original completeness: 91.8%\n",
      "   Final completeness: 100.0%\n",
      "   Improvement: +8.2%\n",
      "\n",
      "üî¢ Column Types:\n",
      "   Numeric columns: 41\n",
      "   Categorical columns: 11\n",
      "   Total columns: 52\n",
      "\n",
      "‚úÖ No missing values remaining!\n",
      "\n",
      "üéØ Ready for Machine Learning:\n",
      "   ML-ready numeric features: 25\n",
      "   Memory usage: 1.11 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä FINAL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üéØ Processing Summary:\")\n",
    "print(f\"   Original shape: {df_raw.shape}\")\n",
    "print(f\"   Final shape: {df_final.shape}\")\n",
    "print(f\"   Rows added/removed: {df_final.shape[0] - df_raw.shape[0]:+d}\")\n",
    "print(f\"   Columns added: {df_final.shape[1] - df_raw.shape[1]:+d}\")\n",
    "print(f\"\\nüìà Data Quality Metrics:\")\n",
    "original_completeness = ((df_raw.shape[0] * df_raw.shape[1] - df_raw.isnull().sum().sum()) / (df_raw.shape[0] * df_raw.shape[1]) * 100)\n",
    "final_completeness = ((df_final.shape[0] * df_final.shape[1] - df_final.isnull().sum().sum()) / (df_final.shape[0] * df_final.shape[1]) * 100)\n",
    "print(f\"   Original completeness: {original_completeness:.1f}%\")\n",
    "print(f\"   Final completeness: {final_completeness:.1f}%\")\n",
    "print(f\"   Improvement: {final_completeness - original_completeness:+.1f}%\")\n",
    "print(f\"\\nüî¢ Column Types:\")\n",
    "numeric_count = len(df_final.select_dtypes(include=[np.number]).columns)\n",
    "categorical_count = len(df_final.select_dtypes(include=['object', 'category']).columns)\n",
    "print(f\"   Numeric columns: {numeric_count}\")\n",
    "print(f\"   Categorical columns: {categorical_count}\")\n",
    "print(f\"   Total columns: {len(df_final.columns)}\")\n",
    "remaining_missing = df_final.isnull().sum().sum()\n",
    "if remaining_missing > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Remaining missing values: {remaining_missing}\")\n",
    "    missing_cols = df_final.columns[df_final.isnull().any()].tolist()\n",
    "    for col in missing_cols[:5]:  # Show top 5\n",
    "        missing_count = df_final[col].isnull().sum()\n",
    "        print(f\"   {col}: {missing_count} missing\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values remaining!\")\n",
    "print(f\"\\nüéØ Ready for Machine Learning:\")\n",
    "ml_ready_cols = [col for col in df_final.columns \n",
    "                if col not in ['Symbol', 'Name', 'Address', 'Description'] \n",
    "                and df_final[col].dtype in [np.number, 'int64', 'float64']]\n",
    "print(f\"   ML-ready numeric features: {len(ml_ready_cols)}\")\n",
    "print(f\"   Memory usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d059c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING PROCESSED DATA\n",
      "==============================\n",
      "‚úÖ Processed data saved to: ../data/processed/processed.csv\n",
      "üìä File size: 1.01 MB\n",
      "‚úÖ Processing metadata saved to: ../data/processed/processing_metadata.txt\n",
      "\n",
      "üéâ DATA PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "üìà Dataset ready for machine learning and advanced analytics\n",
      "üìÅ Processed files location: ../data/processed/\n",
      "‚è±Ô∏è  Processing completed at: 2026-01-26 14:55:58\n"
     ]
    }
   ],
   "source": [
    "print(\"üíæ SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 30)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "processed_file_path = os.path.join(output_path, 'processed.csv')\n",
    "metadata_file_path = os.path.join(output_path, 'processing_metadata.txt')\n",
    "try:\n",
    "    df_final.to_csv(processed_file_path, index=False)\n",
    "    print(f\"‚úÖ Processed data saved to: {processed_file_path}\")\n",
    "    print(f\"üìä File size: {os.path.getsize(processed_file_path) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Save processing metadata\n",
    "    metadata = f\"\"\"ESG SUSTAINABILITY DATA PROCESSING METADATA\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "====================================================\n",
    "\n",
    "ORIGINAL DATA:\n",
    "- Shape: {df_raw.shape}\n",
    "- Completeness: {original_completeness:.1f}%\n",
    "- Missing values: {df_raw.isnull().sum().sum():,}\n",
    "\n",
    "PROCESSED DATA:\n",
    "- Shape: {df_final.shape}\n",
    "- Completeness: {final_completeness:.1f}%\n",
    "- Missing values: {df_final.isnull().sum().sum():,}\n",
    "- ML-ready features: {len(ml_ready_cols)}\n",
    "\n",
    "PROCESSING STEPS:\n",
    "1. ‚úÖ Basic cleaning (duplicates, symbols)\n",
    "2. ‚úÖ Employee data standardization\n",
    "3. ‚úÖ ESG scores preprocessing\n",
    "4. ‚úÖ Categorical data standardization\n",
    "5. ‚úÖ Missing value imputation\n",
    "6. ‚úÖ Outlier detection and treatment\n",
    "7. ‚úÖ Feature engineering\n",
    "8. ‚úÖ Categorical encoding\n",
    "9. ‚úÖ Feature scaling/normalization\n",
    "10. ‚úÖ Final quality assessment\n",
    "\n",
    "NEW FEATURES CREATED:\n",
    "{chr(10).join([f\"- {feature}\" for feature in new_features])}\n",
    "\n",
    "ENCODED COLUMNS:\n",
    "{chr(10).join([f\"- {col}\" for col in new_encoded_cols])}\n",
    "\n",
    "IMPUTATION STRATEGIES USED:\n",
    "{chr(10).join([f\"- {strategy}: {count} columns\" for strategy, count in strategy_counts.items()])}\n",
    "\n",
    "READY FOR:\n",
    "- Machine Learning model training\n",
    "- Statistical analysis\n",
    "- Predictive modeling\n",
    "- ESG risk assessment\n",
    "\"\"\"\n",
    "\n",
    "    with open(metadata_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(metadata)\n",
    "    \n",
    "    print(f\"‚úÖ Processing metadata saved to: {metadata_file_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving files: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ DATA PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"üìà Dataset ready for machine learning and advanced analytics\")\n",
    "print(f\"üìÅ Processed files location: {output_path}\")\n",
    "print(f\"‚è±Ô∏è  Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c4e7a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PROCESSED DATA SAMPLE PREVIEW\n",
      "========================================\n",
      "üìã Final Dataset Shape: (503, 52)\n",
      "\n",
      "üìä First 3 rows of key columns:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Symbol",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sector",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Total ESG Risk score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ESG_Risk_Category",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "Employee_Size_Category",
         "rawType": "category",
         "type": "unknown"
        }
       ],
       "ref": "5bb8876d-017e-436b-b5b2-bd9d0d12bdfa",
       "rows": [
        [
         "0",
         "ENPH",
         "Enphase Energy, Inc.",
         "Technology",
         "-5.584036695917103e-16",
         "Medium",
         "Medium"
        ],
        [
         "1",
         "EMN",
         "Eastman Chemical Company",
         "Basic Materials",
         "0.591971164412737",
         "High",
         "Large"
        ],
        [
         "2",
         "DPZ",
         "Domino's Pizza Inc.",
         "Consumer Cyclical",
         "1.2049601379972756",
         "High",
         "Medium"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Total ESG Risk score</th>\n",
       "      <th>ESG_Risk_Category</th>\n",
       "      <th>Employee_Size_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy, Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>-5.584037e-16</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMN</td>\n",
       "      <td>Eastman Chemical Company</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>5.919712e-01</td>\n",
       "      <td>High</td>\n",
       "      <td>Large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPZ</td>\n",
       "      <td>Domino's Pizza Inc.</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>1.204960e+00</td>\n",
       "      <td>High</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                      Name             Sector  Total ESG Risk score  \\\n",
       "0   ENPH      Enphase Energy, Inc.         Technology         -5.584037e-16   \n",
       "1    EMN  Eastman Chemical Company    Basic Materials          5.919712e-01   \n",
       "2    DPZ       Domino's Pizza Inc.  Consumer Cyclical          1.204960e+00   \n",
       "\n",
       "  ESG_Risk_Category Employee_Size_Category  \n",
       "0            Medium                 Medium  \n",
       "1              High                  Large  \n",
       "2              High                 Medium  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Statistical Summary of Key ESG Metrics:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Total ESG Risk score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Environment Risk Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Governance Risk Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Social Risk Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Controversy Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ESG_Risk_Percentile_Numeric",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "90e8f778-dd7f-4a1c-bcdc-4ca52a8ad270",
       "rows": [
        [
         "count",
         "503.0",
         "503.0",
         "503.0",
         "503.0",
         "503.0",
         "503.0"
        ],
        [
         "mean",
         "-0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "-0.0"
        ],
        [
         "std",
         "1.001",
         "1.001",
         "1.001",
         "1.001",
         "1.001",
         "1.001"
        ],
        [
         "min",
         "-2.269",
         "-1.276",
         "-1.554",
         "-2.394",
         "-1.364",
         "-1.523"
        ],
        [
         "25%",
         "-0.728",
         "-0.859",
         "-0.731",
         "-0.509",
         "-0.254",
         "-0.81"
        ],
        [
         "50%",
         "-0.0",
         "-0.17",
         "-0.32",
         "-0.215",
         "0.024",
         "-0.141"
        ],
        [
         "75%",
         "0.568",
         "0.46",
         "0.458",
         "0.551",
         "0.024",
         "0.662"
        ],
        [
         "max",
         "3.17",
         "3.943",
         "5.944",
         "3.997",
         "4.187",
         "2.579"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total ESG Risk score</th>\n",
       "      <th>Environment Risk Score</th>\n",
       "      <th>Governance Risk Score</th>\n",
       "      <th>Social Risk Score</th>\n",
       "      <th>Controversy Score</th>\n",
       "      <th>ESG_Risk_Percentile_Numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>503.000</td>\n",
       "      <td>503.000</td>\n",
       "      <td>503.000</td>\n",
       "      <td>503.000</td>\n",
       "      <td>503.000</td>\n",
       "      <td>503.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.269</td>\n",
       "      <td>-1.276</td>\n",
       "      <td>-1.554</td>\n",
       "      <td>-2.394</td>\n",
       "      <td>-1.364</td>\n",
       "      <td>-1.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.728</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.568</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.170</td>\n",
       "      <td>3.943</td>\n",
       "      <td>5.944</td>\n",
       "      <td>3.997</td>\n",
       "      <td>4.187</td>\n",
       "      <td>2.579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total ESG Risk score  Environment Risk Score  Governance Risk Score  \\\n",
       "count               503.000                 503.000                503.000   \n",
       "mean                 -0.000                   0.000                  0.000   \n",
       "std                   1.001                   1.001                  1.001   \n",
       "min                  -2.269                  -1.276                 -1.554   \n",
       "25%                  -0.728                  -0.859                 -0.731   \n",
       "50%                  -0.000                  -0.170                 -0.320   \n",
       "75%                   0.568                   0.460                  0.458   \n",
       "max                   3.170                   3.943                  5.944   \n",
       "\n",
       "       Social Risk Score  Controversy Score  ESG_Risk_Percentile_Numeric  \n",
       "count            503.000            503.000                      503.000  \n",
       "mean              -0.000              0.000                       -0.000  \n",
       "std                1.001              1.001                        1.001  \n",
       "min               -2.394             -1.364                       -1.523  \n",
       "25%               -0.509             -0.254                       -0.810  \n",
       "50%               -0.215              0.024                       -0.141  \n",
       "75%                0.551              0.024                        0.662  \n",
       "max                3.997              4.187                        2.579  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Column Categories:\n",
      "   üìä Numeric: 41 columns\n",
      "   üî§ Categorical: 9 columns\n",
      "   üÜï Engineered: 10 columns\n",
      "   üé® Encoded: 26 columns\n",
      "\n",
      "‚úÖ Data preprocessing pipeline completed successfully!\n",
      "üöÄ Ready for model training and advanced analytics\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç PROCESSED DATA SAMPLE PREVIEW\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìã Final Dataset Shape: {df_final.shape}\")\n",
    "print(f\"\\nüìä First 3 rows of key columns:\")\n",
    "key_columns = ['Symbol', 'Name', 'Sector', 'Total ESG Risk score', 'ESG_Risk_Category', 'Employee_Size_Category']\n",
    "display_cols = [col for col in key_columns if col in df_final.columns]\n",
    "if len(display_cols) > 0:\n",
    "    display(df_final[display_cols].head(3))\n",
    "print(f\"\\nüìà Statistical Summary of Key ESG Metrics:\")\n",
    "esg_metrics = [col for col in df_final.columns if any(kw in col.lower() for kw in ['esg', 'risk', 'score']) and df_final[col].dtype in [np.number]]\n",
    "if len(esg_metrics) > 0:\n",
    "    display(df_final[esg_metrics[:6]].describe().round(3))\n",
    "print(f\"\\nüéØ Column Categories:\")\n",
    "print(f\"   üìä Numeric: {len(df_final.select_dtypes(include=[np.number]).columns)} columns\")\n",
    "print(f\"   üî§ Categorical: {len(df_final.select_dtypes(include=['object']).columns)} columns\")\n",
    "print(f\"   üÜï Engineered: {len(new_features)} columns\")\n",
    "print(f\"   üé® Encoded: {len(new_encoded_cols)} columns\")\n",
    "print(f\"\\n‚úÖ Data preprocessing pipeline completed successfully!\")\n",
    "print(f\"üöÄ Ready for model training and advanced analytics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
