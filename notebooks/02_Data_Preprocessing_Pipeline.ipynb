{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2640fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ğŸ“… Data preprocessing started on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/dataset.csv'\n",
    "output_path = '../data/processed/'\n",
    "try:\n",
    "    df_raw = pd.read_csv(data_path)\n",
    "    print(\"âœ… Raw dataset loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Original dataset shape: {df_raw.shape[0]:,} rows Ã— {df_raw.shape[1]} columns\")\n",
    "    print(f\"ğŸ’¾ Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Dataset file not found. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "df = df_raw.copy()\n",
    "print(f\"\\nğŸ”„ Working copy created for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726587d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” INITIAL DATA ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Information:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {col:<25} | Missing: {missing_count:3d} ({missing_pct:5.1f}%)\")\n",
    "print(f\"\\nğŸ“Š Total Missing Values: {df.isnull().sum().sum():,}\")\n",
    "print(f\"ğŸ“Š Data Completeness: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f515195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ STEP 1: BASIC DATA CLEANING\")\n",
    "print(\"=\" * 40)\n",
    "initial_shape = df.shape\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {duplicate_rows}\")\n",
    "if duplicate_rows > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Removed {duplicate_rows} duplicate rows\")\n",
    "print(f\"\\nğŸ“‹ Company Symbol Analysis:\")\n",
    "if 'Symbol' in df.columns:\n",
    "    duplicate_symbols = df['Symbol'].duplicated().sum()\n",
    "    print(f\"Duplicate symbols: {duplicate_symbols}\")\n",
    "    if duplicate_symbols > 0:\n",
    "        print(\"ğŸ” Duplicate symbols found:\")\n",
    "        duplicated_syms = df[df['Symbol'].duplicated(keep=False)]['Symbol'].value_counts()\n",
    "        print(duplicated_syms)\n",
    "    else:\n",
    "        print(\"âœ… All company symbols are unique\")\n",
    "print(f\"\\nğŸ“Š Shape after basic cleaning: {df.shape} (removed {initial_shape[0] - df.shape[0]} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ STEP 2: EMPLOYEE DATA STANDARDIZATION\")\n",
    "print(\"=\" * 45)\n",
    "if 'Full Time Employees' in df.columns:\n",
    "    print(\"Processing employee count data...\")\n",
    "    print(f\"Original data type: {df['Full Time Employees'].dtype}\")\n",
    "    \n",
    "    def clean_employee_count(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        \n",
    "        value_str = str(value).strip()\n",
    "        if value_str == '' or value_str.lower() in ['nan', 'null', 'none']:\n",
    "            return np.nan\n",
    "        \n",
    "        value_clean = re.sub(r'[^0-9.]', '', value_str)\n",
    "        try:\n",
    "            return float(value_clean)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    df['Full Time Employees'] = df['Full Time Employees'].apply(clean_employee_count)\n",
    "    \n",
    "    employee_stats = df['Full Time Employees'].describe()\n",
    "    print(f\"âœ… Employee data standardized:\")\n",
    "    print(f\"   Valid records: {df['Full Time Employees'].count()}/{len(df)}\")\n",
    "    print(f\"   Range: {employee_stats['min']:,.0f} to {employee_stats['max']:,.0f}\")\n",
    "    print(f\"   Median: {employee_stats['50%']:,.0f}\")\n",
    "    \n",
    "    extreme_outliers = df['Full Time Employees'] > 1000000\n",
    "    if extreme_outliers.any():\n",
    "        print(f\"\\nâš ï¸  Large companies (>1M employees):\")\n",
    "        large_companies = df[extreme_outliers][['Symbol', 'Name', 'Full Time Employees']]\n",
    "        for _, row in large_companies.iterrows():\n",
    "            print(f\"   {row['Symbol']}: {row['Full Time Employees']:,.0f} employees\")\n",
    "else:\n",
    "    print(\"âŒ 'Full Time Employees' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb06964",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ STEP 3: ESG RISK SCORES PREPROCESSING\")\n",
    "print(\"=\" * 45)\n",
    "esg_columns = []\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['esg', 'risk', 'score', 'environment', 'social', 'governance']):\n",
    "        if df[col].dtype in ['int64', 'float64'] or col in ['Total ESG Risk score', 'Environment Risk Score', 'Governance Risk Score', 'Social Risk Score', 'Controversy Score']:\n",
    "            esg_columns.append(col)\n",
    "print(f\"ğŸ¯ Identified {len(esg_columns)} ESG-related numeric columns:\")\n",
    "for i, col in enumerate(esg_columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "if esg_columns:\n",
    "    print(f\"\\nğŸ“Š ESG Data Completeness Analysis:\")\n",
    "    for col in esg_columns:\n",
    "        valid_count = df[col].count()\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        print(f\"   {col:<25}: {valid_count:3d}/{len(df)} valid ({100-missing_pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ” ESG Score Ranges:\")\n",
    "    for col in esg_columns:\n",
    "        if df[col].count() > 0:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            mean_val = df[col].mean()\n",
    "            print(f\"   {col:<25}: {min_val:6.1f} - {max_val:6.1f} (mean: {mean_val:6.1f})\")\n",
    "else:\n",
    "    print(\"âŒ No ESG columns identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e73d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ STEP 4: CATEGORICAL DATA STANDARDIZATION\")\n",
    "print(\"=\" * 45)\n",
    "categorical_columns = ['Sector', 'Industry', 'Controversy Level', 'ESG Risk Level', 'ESG Risk Percentile']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nğŸ“Š Processing {col}:\")\n",
    "        \n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace(['nan', 'None', 'null', ''], np.nan)\n",
    "        \n",
    "        unique_values = df[col].value_counts(dropna=False)\n",
    "        print(f\"   Unique values: {len(unique_values)} (including NaN)\")\n",
    "        print(f\"   Missing: {df[col].isnull().sum()} ({(df[col].isnull().sum()/len(df)*100):.1f}%)\")\n",
    "        \n",
    "        if col == 'ESG Risk Percentile':\n",
    "            print(f\"   Sample values: {list(unique_values.head().index)}\")\n",
    "            \n",
    "            def extract_percentile_number(value):\n",
    "                if pd.isna(value) or value == 'nan':\n",
    "                    return np.nan\n",
    "                try:\n",
    "                    percentile_match = re.search(r'(\\d+)', str(value))\n",
    "                    if percentile_match:\n",
    "                        return int(percentile_match.group(1))\n",
    "                    return np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "            \n",
    "            df['ESG_Risk_Percentile_Numeric'] = df[col].apply(extract_percentile_number)\n",
    "            valid_percentiles = df['ESG_Risk_Percentile_Numeric'].dropna()\n",
    "            if len(valid_percentiles) > 0:\n",
    "                print(f\"   âœ… Created numeric percentile column: {valid_percentiles.min():.0f}-{valid_percentiles.max():.0f} range\")\n",
    "        \n",
    "        elif col in ['Controversy Level', 'ESG Risk Level']:\n",
    "            print(f\"   Top categories: {list(unique_values.head().index)}\")\n",
    "        \n",
    "        elif col in ['Sector', 'Industry']:\n",
    "            print(f\"   Top 5 categories: {list(unique_values.head().index)}\")\n",
    "    else:\n",
    "        print(f\"   âŒ {col} not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ STEP 5: MISSING VALUE TREATMENT STRATEGY\")\n",
    "print(\"=\" * 50)\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Data_Type': df.dtypes\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "high_missing = missing_analysis[missing_analysis['Missing_Percentage'] > 10]\n",
    "medium_missing = missing_analysis[(missing_analysis['Missing_Percentage'] > 1) & (missing_analysis['Missing_Percentage'] <= 10)]\n",
    "low_missing = missing_analysis[(missing_analysis['Missing_Percentage'] > 0) & (missing_analysis['Missing_Percentage'] <= 1)]\n",
    "print(f\"ğŸ“Š Missing Value Categories:\")\n",
    "print(f\"   ğŸ”´ High missing (>10%): {len(high_missing)} columns\")\n",
    "print(f\"   ğŸŸ¡ Medium missing (1-10%): {len(medium_missing)} columns\")\n",
    "print(f\"   ğŸŸ¢ Low missing (<1%): {len(low_missing)} columns\")\n",
    "if len(high_missing) > 0:\n",
    "    print(f\"\\nğŸ”´ High Missing Columns:\")\n",
    "    for _, row in high_missing.iterrows():\n",
    "        print(f\"   {row['Column']:<25}: {row['Missing_Percentage']:5.1f}% missing\")\n",
    "imputation_strategy = {}\n",
    "for col in df.columns:\n",
    "    missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    if missing_pct == 0:\n",
    "        imputation_strategy[col] = 'no_action'\n",
    "    elif missing_pct > 25:\n",
    "        imputation_strategy[col] = 'flag_and_median'\n",
    "    elif missing_pct > 10:\n",
    "        if col in esg_columns:\n",
    "            imputation_strategy[col] = 'knn_imputation'\n",
    "        else:\n",
    "            imputation_strategy[col] = 'median_mode'\n",
    "    elif missing_pct > 1:\n",
    "        imputation_strategy[col] = 'median_mode'\n",
    "    else:\n",
    "        imputation_strategy[col] = 'forward_fill'\n",
    "print(f\"\\nğŸ“‹ Imputation Strategy Summary:\")\n",
    "strategy_counts = pd.Series(list(imputation_strategy.values())).value_counts()\n",
    "for strategy, count in strategy_counts.items():\n",
    "    print(f\"   {strategy}: {count} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ STEP 6: IMPLEMENTING MISSING VALUE IMPUTATION\")\n",
    "print(\"=\" * 50)\n",
    "df_imputed = df.copy()\n",
    "for col, strategy in imputation_strategy.items():\n",
    "    if strategy == 'no_action':\n",
    "        continue\n",
    "    \n",
    "    missing_before = df_imputed[col].isnull().sum()\n",
    "    \n",
    "    if strategy == 'forward_fill':\n",
    "        if df_imputed[col].dtype in ['object']:\n",
    "            df_imputed[col].fillna(method='ffill', inplace=True)\n",
    "            df_imputed[col].fillna('Unknown', inplace=True)\n",
    "        else:\n",
    "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    elif strategy == 'median_mode':\n",
    "        if df_imputed[col].dtype in ['object']:\n",
    "            mode_val = df_imputed[col].mode()\n",
    "            fill_val = mode_val.iloc[0] if len(mode_val) > 0 else 'Unknown'\n",
    "            df_imputed[col].fillna(fill_val, inplace=True)\n",
    "        else:\n",
    "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    elif strategy == 'flag_and_median':\n",
    "        df_imputed[f'{col}_was_missing'] = df_imputed[col].isnull().astype(int)\n",
    "        if df_imputed[col].dtype in ['object']:\n",
    "            mode_val = df_imputed[col].mode()\n",
    "            fill_val = mode_val.iloc[0] if len(mode_val) > 0 else 'Unknown'\n",
    "            df_imputed[col].fillna(fill_val, inplace=True)\n",
    "        else:\n",
    "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    elif strategy == 'knn_imputation' and col in esg_columns:\n",
    "        if df_imputed[col].count() > 0:\n",
    "            esg_subset = df_imputed[esg_columns].select_dtypes(include=[np.number])\n",
    "            if esg_subset.shape[1] > 1:\n",
    "                try:\n",
    "                    imputer = KNNImputer(n_neighbors=5)\n",
    "                    esg_imputed = imputer.fit_transform(esg_subset)\n",
    "                    \n",
    "                    col_idx = esg_subset.columns.get_loc(col)\n",
    "                    df_imputed[col] = esg_imputed[:, col_idx]\n",
    "                except:\n",
    "                    df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "            else:\n",
    "                df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
    "    \n",
    "    missing_after = df_imputed[col].isnull().sum()\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        print(f\"   {col:<30}: {missing_before:3d} â†’ {missing_after:3d} missing ({strategy})\")\n",
    "\n",
    "print(f\"\\nâœ… Imputation completed!\")\n",
    "print(f\"ğŸ“Š Total missing values: {df.isnull().sum().sum()} â†’ {df_imputed.isnull().sum().sum()}\")\n",
    "print(f\"ğŸ“Š Data completeness: {((df_imputed.shape[0] * df_imputed.shape[1] - df_imputed.isnull().sum().sum()) / (df_imputed.shape[0] * df_imputed.shape[1]) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ STEP 7: OUTLIER DETECTION AND TREATMENT\")\n",
    "print(\"=\" * 45)\n",
    "numeric_columns = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = []\n",
    "for col in numeric_columns:\n",
    "    if df_imputed[col].count() > 0:\n",
    "        Q1 = df_imputed[col].quantile(0.25)\n",
    "        Q3 = df_imputed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df_imputed[(df_imputed[col] < lower_bound) | (df_imputed[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df_imputed)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outliers': outlier_count,\n",
    "            'Percentage': outlier_percentage,\n",
    "            'Lower_Bound': lower_bound,\n",
    "            'Upper_Bound': upper_bound\n",
    "        })\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df.sort_values('Percentage', ascending=False)\n",
    "print(\"ğŸ“Š Outlier Analysis:\")\n",
    "for _, row in outlier_df.head(10).iterrows():\n",
    "    if row['Outliers'] > 0:\n",
    "        print(f\"   {row['Column']:<30}: {row['Outliers']:3d} outliers ({row['Percentage']:4.1f}%)\")\n",
    "print(f\"\\nğŸ¯ Outlier Treatment Strategy:\")\n",
    "for col in numeric_columns:\n",
    "    if col in outlier_df[outlier_df['Percentage'] > 5]['Column'].values:\n",
    "        if 'Employee' in col:\n",
    "            cap_value = df_imputed[col].quantile(0.99)\n",
    "            outlier_count = (df_imputed[col] > cap_value).sum()\n",
    "            df_imputed[col] = np.where(df_imputed[col] > cap_value, cap_value, df_imputed[col])\n",
    "            print(f\"   {col}: Capped {outlier_count} values at 99th percentile ({cap_value:,.0f})\")\n",
    "        elif col in esg_columns:\n",
    "            outlier_count = len(outlier_df[outlier_df['Column'] == col])\n",
    "            if outlier_count > 0:\n",
    "                print(f\"   {col}: Keeping {outlier_df[outlier_df['Column']==col]['Outliers'].iloc[0]} outliers (legitimate ESG risk variation)\")\n",
    "        else:\n",
    "            outlier_info = outlier_df[outlier_df['Column'] == col].iloc[0]\n",
    "            if outlier_info['Outliers'] > 0:\n",
    "                print(f\"   {col}: Flagged {outlier_info['Outliers']} outliers for review\")\n",
    "\n",
    "print(f\"\\nâœ… Outlier treatment completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23869d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ STEP 8: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 35)\n",
    "df_featured = df_imputed.copy()\n",
    "if len([col for col in esg_columns if col in df_featured.columns]) >= 3:\n",
    "    if all(col in df_featured.columns for col in ['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']):\n",
    "        df_featured['ESG_Component_Balance'] = df_featured[['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']].std(axis=1)\n",
    "        print(\"âœ… Created ESG_Component_Balance (std of E, S, G scores)\")\n",
    "        \n",
    "        df_featured['ESG_Max_Component'] = df_featured[['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']].max(axis=1)\n",
    "        df_featured['ESG_Min_Component'] = df_featured[['Environment Risk Score', 'Social Risk Score', 'Governance Risk Score']].min(axis=1)\n",
    "        print(\"âœ… Created ESG_Max_Component and ESG_Min_Component\")\n",
    "if 'Full Time Employees' in df_featured.columns:\n",
    "    df_featured['Employee_Size_Category'] = pd.cut(\n",
    "        df_featured['Full Time Employees'], \n",
    "        bins=[0, 1000, 10000, 50000, float('inf')], \n",
    "        labels=['Small', 'Medium', 'Large', 'Enterprise'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    print(\"âœ… Created Employee_Size_Category (Small/Medium/Large/Enterprise)\")\n",
    "    \n",
    "    df_featured['Log_Employees'] = np.log1p(df_featured['Full Time Employees'])\n",
    "    print(\"âœ… Created Log_Employees (log-transformed employee count)\")\n",
    "if 'Total ESG Risk score' in df_featured.columns:\n",
    "    esg_risk_mean = df_featured['Total ESG Risk score'].mean()\n",
    "    df_featured['ESG_Risk_Above_Average'] = (df_featured['Total ESG Risk score'] > esg_risk_mean).astype(int)\n",
    "    print(f\"âœ… Created ESG_Risk_Above_Average (threshold: {esg_risk_mean:.1f})\")\n",
    "    \n",
    "    df_featured['ESG_Risk_Category'] = pd.cut(\n",
    "        df_featured['Total ESG Risk score'],\n",
    "        bins=[0, 15, 25, 35, float('inf')],\n",
    "        labels=['Low', 'Medium', 'High', 'Severe'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    print(\"âœ… Created ESG_Risk_Category (Low/Medium/High/Severe)\")\n",
    "if 'Sector' in df_featured.columns:\n",
    "    sector_risk_avg = df_featured.groupby('Sector')['Total ESG Risk score'].mean()\n",
    "    df_featured['Sector_Risk_Average'] = df_featured['Sector'].map(sector_risk_avg)\n",
    "    df_featured['ESG_vs_Sector_Average'] = df_featured['Total ESG Risk score'] - df_featured['Sector_Risk_Average']\n",
    "    print(\"âœ… Created Sector_Risk_Average and ESG_vs_Sector_Average\")\n",
    "if 'ESG Risk Percentile' in df_featured.columns and 'ESG_Risk_Percentile_Numeric' in df_featured.columns:\n",
    "    df_featured['High_Risk_Percentile'] = (df_featured['ESG_Risk_Percentile_Numeric'] > 75).astype(int)\n",
    "    print(\"âœ… Created High_Risk_Percentile (>75th percentile flag)\")\n",
    "new_features = [col for col in df_featured.columns if col not in df_imputed.columns]\n",
    "print(f\"\\nğŸ“Š Feature Engineering Summary:\")\n",
    "print(f\"   Original features: {len(df_imputed.columns)}\")\n",
    "print(f\"   New features: {len(new_features)}\")\n",
    "print(f\"   Total features: {len(df_featured.columns)}\")\n",
    "if new_features:\n",
    "    print(f\"\\nğŸ†• New features created:\")\n",
    "    for i, feature in enumerate(new_features, 1):\n",
    "        print(f\"   {i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ STEP 9: ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\" * 45)\n",
    "df_encoded = df_featured.copy()\n",
    "categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['Symbol', 'Name', 'Address', 'Description']]\n",
    "print(f\"ğŸ“Š Categorical columns to encode: {len(categorical_cols)}\")\n",
    "if len(categorical_cols) > 0:\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nğŸ”¤ Processing {col}:\")\n",
    "        unique_count = df_encoded[col].nunique()\n",
    "        print(f\"   Unique values: {unique_count}\")\n",
    "        \n",
    "        if unique_count <= 10:\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=f'{col}', dummy_na=True)\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            print(f\"   âœ… One-hot encoded into {len(dummies.columns)} columns\")\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[f'{col}_Encoded'] = le.fit_transform(df_encoded[col].fillna('Unknown'))\n",
    "            print(f\"   âœ… Label encoded as {col}_Encoded\")\n",
    "        \n",
    "        print(f\"   ğŸ“ Keeping original {col} for reference\")\n",
    "print(f\"\\nğŸ“Š Encoding Summary:\")\n",
    "print(f\"   Columns after encoding: {len(df_encoded.columns)}\")\n",
    "new_encoded_cols = [col for col in df_encoded.columns if col not in df_featured.columns]\n",
    "print(f\"   New encoded columns: {len(new_encoded_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9abf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ STEP 10: FEATURE SCALING AND NORMALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "df_final = df_encoded.copy()\n",
    "numeric_cols_to_scale = df_final.select_dtypes(include=[np.number]).columns\n",
    "id_cols = ['Symbol', 'Name', 'Address', 'Description']\n",
    "exclude_from_scaling = ['Symbol', 'Name', 'Address', 'Description'] + \\\n",
    "                      [col for col in df_final.columns if '_was_missing' in col or \n",
    "                       col.endswith('_Encoded') or \n",
    "                       col.startswith(tuple(['Sector_', 'Industry_', 'Controversy Level_', 'ESG Risk Level_']))]\n",
    "scale_cols = [col for col in numeric_cols_to_scale if col not in exclude_from_scaling]\n",
    "print(f\"ğŸ“Š Columns to scale: {len(scale_cols)}\")\n",
    "print(f\"   ESG-related: {[col for col in scale_cols if any(kw in col.lower() for kw in ['esg', 'risk', 'score', 'environment', 'social', 'governance'])]}\")\n",
    "print(f\"   Other numeric: {[col for col in scale_cols if not any(kw in col.lower() for kw in ['esg', 'risk', 'score', 'environment', 'social', 'governance'])]}\")\n",
    "if len(scale_cols) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    df_final[scale_cols] = scaler.fit_transform(df_final[scale_cols])\n",
    "    print(f\"\\nâœ… Standardized {len(scale_cols)} numeric columns (mean=0, std=1)\")\n",
    "    \n",
    "    scaling_stats = pd.DataFrame({\n",
    "        'Mean': df_final[scale_cols].mean(),\n",
    "        'Std': df_final[scale_cols].std(),\n",
    "        'Min': df_final[scale_cols].min(),\n",
    "        'Max': df_final[scale_cols].max()\n",
    "    })\n",
    "    print(f\"\\nğŸ“Š Post-scaling statistics:\")\n",
    "    print(f\"   Mean range: {scaling_stats['Mean'].min():.3f} to {scaling_stats['Mean'].max():.3f}\")\n",
    "    print(f\"   Std range: {scaling_stats['Std'].min():.3f} to {scaling_stats['Std'].max():.3f}\")\n",
    "else:\n",
    "    print(\"âŒ No columns found for scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š FINAL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"ğŸ¯ Processing Summary:\")\n",
    "print(f\"   Original shape: {df_raw.shape}\")\n",
    "print(f\"   Final shape: {df_final.shape}\")\n",
    "print(f\"   Rows added/removed: {df_final.shape[0] - df_raw.shape[0]:+d}\")\n",
    "print(f\"   Columns added: {df_final.shape[1] - df_raw.shape[1]:+d}\")\n",
    "print(f\"\\nğŸ“ˆ Data Quality Metrics:\")\n",
    "original_completeness = ((df_raw.shape[0] * df_raw.shape[1] - df_raw.isnull().sum().sum()) / (df_raw.shape[0] * df_raw.shape[1]) * 100)\n",
    "final_completeness = ((df_final.shape[0] * df_final.shape[1] - df_final.isnull().sum().sum()) / (df_final.shape[0] * df_final.shape[1]) * 100)\n",
    "print(f\"   Original completeness: {original_completeness:.1f}%\")\n",
    "print(f\"   Final completeness: {final_completeness:.1f}%\")\n",
    "print(f\"   Improvement: {final_completeness - original_completeness:+.1f}%\")\n",
    "print(f\"\\nğŸ”¢ Column Types:\")\n",
    "numeric_count = len(df_final.select_dtypes(include=[np.number]).columns)\n",
    "categorical_count = len(df_final.select_dtypes(include=['object', 'category']).columns)\n",
    "print(f\"   Numeric columns: {numeric_count}\")\n",
    "print(f\"   Categorical columns: {categorical_count}\")\n",
    "print(f\"   Total columns: {len(df_final.columns)}\")\n",
    "remaining_missing = df_final.isnull().sum().sum()\n",
    "if remaining_missing > 0:\n",
    "    print(f\"\\nâš ï¸  Remaining missing values: {remaining_missing}\")\n",
    "    missing_cols = df_final.columns[df_final.isnull().any()].tolist()\n",
    "    for col in missing_cols[:5]:\n",
    "        missing_count = df_final[col].isnull().sum()\n",
    "        print(f\"   {col}: {missing_count} missing\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No missing values remaining!\")\n",
    "print(f\"\\nğŸ¯ Ready for Machine Learning:\")\n",
    "ml_ready_cols = [col for col in df_final.columns \n",
    "                if col not in ['Symbol', 'Name', 'Address', 'Description'] \n",
    "                and df_final[col].dtype in [np.number, 'int64', 'float64']]\n",
    "print(f\"   ML-ready numeric features: {len(ml_ready_cols)}\")\n",
    "print(f\"   Memory usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’¾ SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 30)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "processed_file_path = os.path.join(output_path, 'processed.csv')\n",
    "metadata_file_path = os.path.join(output_path, 'processing_metadata.txt')\n",
    "try:\n",
    "    df_final.to_csv(processed_file_path, index=False)\n",
    "    print(f\"âœ… Processed data saved to: {processed_file_path}\")\n",
    "    print(f\"ğŸ“Š File size: {os.path.getsize(processed_file_path) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    metadata = f\"\"\"ESG SUSTAINABILITY DATA PROCESSING METADATA\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "====================================================\n",
    "\n",
    "ORIGINAL DATA:\n",
    "- Shape: {df_raw.shape}\n",
    "- Completeness: {original_completeness:.1f}%\n",
    "- Missing values: {df_raw.isnull().sum().sum():,}\n",
    "\n",
    "PROCESSED DATA:\n",
    "- Shape: {df_final.shape}\n",
    "- Completeness: {final_completeness:.1f}%\n",
    "- Missing values: {df_final.isnull().sum().sum():,}\n",
    "- ML-ready features: {len(ml_ready_cols)}\n",
    "\n",
    "PROCESSING STEPS:\n",
    "1. âœ… Basic cleaning (duplicates, symbols)\n",
    "2. âœ… Employee data standardization\n",
    "3. âœ… ESG scores preprocessing\n",
    "4. âœ… Categorical data standardization\n",
    "5. âœ… Missing value imputation\n",
    "6. âœ… Outlier detection and treatment\n",
    "7. âœ… Feature engineering\n",
    "8. âœ… Categorical encoding\n",
    "9. âœ… Feature scaling/normalization\n",
    "10. âœ… Final quality assessment\n",
    "\n",
    "NEW FEATURES CREATED:\n",
    "{chr(10).join([f\"- {feature}\" for feature in new_features])}\n",
    "\n",
    "ENCODED COLUMNS:\n",
    "{chr(10).join([f\"- {col}\" for col in new_encoded_cols])}\n",
    "\n",
    "IMPUTATION STRATEGIES USED:\n",
    "{chr(10).join([f\"- {strategy}: {count} columns\" for strategy, count in strategy_counts.items()])}\n",
    "\n",
    "READY FOR:\n",
    "- Machine Learning model training\n",
    "- Statistical analysis\n",
    "- Predictive modeling\n",
    "- ESG risk assessment\n",
    "\"\"\"\n",
    "\n",
    "    with open(metadata_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(metadata)\n",
    "    \n",
    "    print(f\"âœ… Processing metadata saved to: {metadata_file_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving files: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ DATA PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"ğŸ“ˆ Dataset ready for machine learning and advanced analytics\")\n",
    "print(f\"ğŸ“ Processed files location: {output_path}\")\n",
    "print(f\"â±ï¸  Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” PROCESSED DATA SAMPLE PREVIEW\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ğŸ“‹ Final Dataset Shape: {df_final.shape}\")\n",
    "print(f\"\\nğŸ“Š First 3 rows of key columns:\")\n",
    "key_columns = ['Symbol', 'Name', 'Sector', 'Total ESG Risk score', 'ESG_Risk_Category', 'Employee_Size_Category']\n",
    "display_cols = [col for col in key_columns if col in df_final.columns]\n",
    "if len(display_cols) > 0:\n",
    "    display(df_final[display_cols].head(3))\n",
    "print(f\"\\nğŸ“ˆ Statistical Summary of Key ESG Metrics:\")\n",
    "esg_metrics = [col for col in df_final.columns if any(kw in col.lower() for kw in ['esg', 'risk', 'score']) and df_final[col].dtype in [np.number]]\n",
    "if len(esg_metrics) > 0:\n",
    "    display(df_final[esg_metrics[:6]].describe().round(3))\n",
    "print(f\"\\nğŸ¯ Column Categories:\")\n",
    "print(f\"   ğŸ“Š Numeric: {len(df_final.select_dtypes(include=[np.number]).columns)} columns\")\n",
    "print(f\"   ğŸ”¤ Categorical: {len(df_final.select_dtypes(include=['object']).columns)} columns\")\n",
    "print(f\"   ğŸ†• Engineered: {len(new_features)} columns\")\n",
    "print(f\"   ğŸ¨ Encoded: {len(new_encoded_cols)} columns\")\n",
    "print(f\"\\nâœ… Data preprocessing pipeline completed successfully!\")\n",
    "print(f\"ğŸš€ Ready for model training and advanced analytics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
